{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multinomial_Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Data-Science-and-Data-Analytics-Courses/MITx---Machine-Learning-with-Python-From-Linear-Models-to-Deep-Learning-Jun-11-2019/blob/master/Project%202%3A%20Digit%20recognition%20(Part%201)/Multinomial_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jKf749g6256",
        "colab_type": "text"
      },
      "source": [
        "# Multinomial (Softmax) Regression and Gradient Descent\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiqX2dH_HnE0",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xw3SOTRf9Nz_",
        "outputId": "a9321e12-8d33-453b-d6dc-dbb5b65c2ae2",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Notebook Library\n",
        "url = \"https://github.com/Data-Science-and-Data-Analytics-Courses/Notebook-Library\"\n",
        "repo = Path(\"/nblib\")\n",
        "!git clone \"{url}\" \"{repo}\"\n",
        "if repo.parent.as_posix() not in sys.path:\n",
        "  sys.path.append(repo.parent.as_posix())\n",
        "%run \"{repo}/.Importable.ipynb\"\n",
        "\n",
        "from nblib import Git\n",
        "# Remote\n",
        "URL = \"https://github.com/Data-Science-and-Data-Analytics-Courses/MITx---Machine-Learning-with-Python-From-Linear-Models-to-Deep-Learning-Jun-11-2019\"\n",
        "REPO = Git.clone(URL, dest=\"/content\")\n",
        "if REPO.as_posix() not in sys.path:\n",
        "  sys.path.append(REPO.as_posix())\n",
        "\n",
        "# Working directory, for running modules in part1\n",
        "part1dir = REPO / \"Project 2: Digit recognition (Part 1)/mnist/part1\"\n",
        "os.chdir(part1dir)\n",
        "\n",
        "from setup.Setup import *\n",
        "import main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9-EETewelSI",
        "colab_type": "text"
      },
      "source": [
        "## Test Error on Softmax Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P7VGVrIH7BEF",
        "colab": {}
      },
      "source": [
        "def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n",
        "    \"\"\"\n",
        "    Computes the total cost over every datapoint.\n",
        "\n",
        "    Args:\n",
        "        X - (n, d) NumPy array (n datapoints each with d features)\n",
        "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
        "            data point\n",
        "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
        "                model for label j\n",
        "        lambda_factor - the regularization constant (scalar)\n",
        "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
        "\n",
        "    Returns\n",
        "        c - the cost value (scalar)\n",
        "    \"\"\"\n",
        "    #YOUR CODE HERE\n",
        "    n, d = X.shape # datapoints, features\n",
        "    prob = compute_probabilities(X, theta, temp_parameter)\n",
        "    loss = -np.log(prob[Y, range(n)]).mean() # only account prob value for label Y(i)\n",
        "    reg = 1/2 * lambda_factor * np.sum(theta**2) # regularization term\n",
        "    \n",
        "    return loss + reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n3CRE5Ko7BD5",
        "colab": {}
      },
      "source": [
        "def compute_probabilities(X, theta, temp_parameter):\n",
        "    \"\"\"\n",
        "    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n",
        "    for j = 0, 1, ..., k-1\n",
        "    Args:\n",
        "        X - (n, d) NumPy array (n datapoints each with d features)\n",
        "        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n",
        "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
        "    Returns:\n",
        "        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n",
        "    \"\"\"\n",
        "    #YOUR CODE HERE\n",
        "    n, d = X.shape\n",
        "    k = theta.shape[0] # number of labels\n",
        "    \n",
        "    H = np.zeros((k, n))\n",
        "    for i in range(n): # each data point\n",
        "        # Linear transformation\n",
        "        z = theta.dot(X[i]) / temp_parameter\n",
        "        z -= np.max(z) # keep the resulting number from getting too large\n",
        "    \n",
        "        # Softmax\n",
        "        h = np.exp(z)\n",
        "        h /= np.sum(h)\n",
        "        H[:, i] = h\n",
        "    \n",
        "    return H"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b253383f-b6c2-4f88-f00f-87dc795d4055",
        "id": "iBjqrXvW7A0I",
        "colab": {}
      },
      "source": [
        "n, d, k = 3, 5, 7\n",
        "X = np.arange(0, n * d).reshape(n, d)\n",
        "Y = np.arange(0, n)\n",
        "zeros = np.zeros((k, d))\n",
        "alpha = 2\n",
        "temp = 0.2\n",
        "lambda_factor = 0.5\n",
        "theta = np.zeros((k, d))\n",
        "exp_res = np.zeros((k, d))\n",
        "exp_res = np.array([\n",
        "[ -7.14285714,  -5.23809524,  -3.33333333,  -1.42857143, 0.47619048],\n",
        "[  9.52380952,  11.42857143,  13.33333333,  15.23809524, 17.14285714],\n",
        "[ 26.19047619,  28.0952381 ,  30.        ,  31.9047619 , 33.80952381],\n",
        "[ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286],\n",
        "[ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286],\n",
        "[ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286],\n",
        "[ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286]\n",
        "])\n",
        "print(x)\n",
        "print(Y)\n",
        "run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OlVr7sq-7Aw-",
        "colab": {}
      },
      "source": [
        "def run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):\n",
        "    \"\"\"\n",
        "    Runs one step of batch gradient descent\n",
        "\n",
        "    Args:\n",
        "        X - (n, d) NumPy array (n datapoints each with d features)\n",
        "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
        "            data point\n",
        "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
        "                model for label j\n",
        "        alpha - the learning rate (scalar)\n",
        "        lambda_factor - the regularization constant (scalar)\n",
        "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
        "\n",
        "    Returns:\n",
        "        theta - (k, d) NumPy array that is the final value of parameters theta\n",
        "    \"\"\"\n",
        "    #YOUR CODE HERE\n",
        "    n, d = X.shape\n",
        "    k = theta.shape[0]\n",
        "    \n",
        "    Yoh = sparse.coo_matrix(([1]*n, (Y, range(n))), shape=(k,n)).toarray() # labels in one vs. all (one hot) format\n",
        "    prob_app = Yoh*1 - compute_probabilities(X, theta, temp_parameter) # probabilities applicable to each label\n",
        "    grad = np.zeros(theta.shape)\n",
        "    loss_grad = -1/(temp_parameter*n) * np.sum(prob_app[:, None] * X, axis=1)\n",
        "    reg_grad = lambda_factor * theta\n",
        "\n",
        "#     for i in range(k): # each label\n",
        "#         loss_grad = -1/(temp_parameter*n) * np.sum(prob_app[i][:, None] * X, axis=0)\n",
        "#         grad[i] = loss_grad + reg_grad\n",
        "    \n",
        "    # Update\n",
        "    theta -= alpha * (loss_grad + reg_grad)\n",
        "    \n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}